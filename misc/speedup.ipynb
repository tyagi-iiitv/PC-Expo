{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from scipy import stats\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSliderData(col1, col2, percent, bi_hist, xed, col1_id, col2_id):\n",
    "    '''\n",
    "    col1 = name of first feature\n",
    "    col2 = name of second feature\n",
    "    percent = Size of the sliding window, percent of total range of col1 to be considered\n",
    "    bi_hist = 2D histogram for col1 and col2 with each cell measuring how many values go from bin1 to bin2\n",
    "    xed = bin values for col1\n",
    "    '''\n",
    "    global lookup_info, num_bins, num_df, eps\n",
    "    var_range = num_df[col1].max() - num_df[col1].min()\n",
    "    # Sliding window size\n",
    "    window_size = percent/100*var_range\n",
    "    percent_id = int(percent/10-1)\n",
    "    # Split the feature into 256 values, the sliding window is iterated over these points\n",
    "    x_pts = np.linspace(num_df[col1].min(), num_df[col1].max(), num_bins)\n",
    "    # calculate outliers for Col1, this returns the index of values which are outliers from num_df[col1]\n",
    "    outlier_ids = np.where(np.absolute(stats.zscore(num_df[col1])) > 2)[0]\n",
    "    # Iterating over each sliding window \n",
    "    for x in range(len(x_pts)):\n",
    "        # Calculating which bins from 2D histogram are in the current sliding window\n",
    "        xbin_ids = np.where(np.logical_and(xed>=x_pts[x], xed<=x_pts[x]+window_size))\n",
    "        # Get point index values in the current sliding window\n",
    "        cur_pts = np.where((num_df[col1] >= x_pts[x]) & \n",
    "                    (num_df[col1] <= x_pts[x]+window_size))\n",
    "        scaling_factor = len(cur_pts[0])/num_df.shape[0]\n",
    "        # Calculate number of outlier points in current sliding window\n",
    "        lookup_info[col1_id, col2_id, percent_id, 11, x] = len(list(set(cur_pts[0]).intersection(outlier_ids)))\n",
    "        # This is the formula for convergence\n",
    "        conv_data = bi_hist[xbin_ids,:]\n",
    "        lookup_info[col1_id, col2_id, percent_id, 6, x] = (conv_data > 0).sum()*scaling_factor\n",
    "        # Only calculate pcp properties if current sliding window has more than 10 points\n",
    "        if len(cur_pts[0]) > 10:\n",
    "            data = num_df.iloc[cur_pts]\n",
    "            matrix = data[[col1, col2]]\n",
    "            # Calculating KL divergence for PCP Neighbor Retrieval Technique (PCP-NR)\n",
    "            # Clear Grouping Calculation\n",
    "            x_pts_pcpnr = list(matrix[col1])\n",
    "            y_pts_pcpnr = list(matrix[col2])\n",
    "            try:\n",
    "                density_x = stats.gaussian_kde(x_pts_pcpnr)(x_pts_pcpnr)\n",
    "                density_y = stats.gaussian_kde(y_pts_pcpnr)(y_pts_pcpnr)\n",
    "                lookup_info[col1_id, col2_id, percent_id, 9, x] = stats.entropy(density_x, density_y)*scaling_factor\n",
    "            except:\n",
    "                pass\n",
    "            sorted_x = sorted(x_pts_pcpnr)\n",
    "            sorted_y = sorted(y_pts_pcpnr)\n",
    "            sigma_x = (sorted_x[-1] - sorted_x[0])/10\n",
    "            sigma_y = (sorted_y[-1] - sorted_y[0])/10\n",
    "            pair_dist_x = np.array([x_pts_pcpnr])-np.array([x_pts_pcpnr]).T\n",
    "            pair_dist_y = np.array([y_pts_pcpnr])-np.array([y_pts_pcpnr]).T\n",
    "            sq_sigma_dists_x = np.exp(-(np.square(pair_dist_x)/sigma_x**2))\n",
    "            sq_sigma_dists_y = np.exp(-(np.square(pair_dist_y)/sigma_y**2))\n",
    "            pji_x = sq_sigma_dists_x/sq_sigma_dists_x.sum(axis=1, keepdims=True)\n",
    "            pji_y = sq_sigma_dists_y/sq_sigma_dists_y.sum(axis=1, keepdims=True)\n",
    "            dkl = pji_x*np.log(pji_x/pji_y)\n",
    "            dkl = dkl.sum() - dkl.trace()\n",
    "            lookup_info[col1_id, col2_id, percent_id, 8, x] = dkl*scaling_factor\n",
    "            # Calculating Corr, Var, and Skewness\n",
    "            (cur_corr, _) = stats.pearsonr(matrix[col1], matrix[col2])\n",
    "            cur_var = np.corrcoef(matrix.T)[0,1]\n",
    "            cur_skew = stats.skew(matrix)[0]\n",
    "            # Calculating Neighborhood (aka parallelism (para))\n",
    "            cur_para = matrix[col2] - matrix[col1]\n",
    "            cur_para = minmax_scale(cur_para)\n",
    "            lookup_info[col1_id, col2_id, percent_id, 7, x] = 1-stats.iqr(cur_para)*scaling_factor\n",
    "            # Separately assigning pos and neg corr/var/skewness\n",
    "            if cur_corr > 0:\n",
    "                lookup_info[col1_id, col2_id, percent_id, 0, x] = cur_corr*scaling_factor\n",
    "            else:\n",
    "                lookup_info[col1_id, col2_id, percent_id, 1, x] = -1*cur_corr*scaling_factor\n",
    "            if cur_var > 0:\n",
    "                lookup_info[col1_id, col2_id, percent_id, 2, x] = cur_var*scaling_factor\n",
    "            else:\n",
    "                lookup_info[col1_id, col2_id, percent_id, 3, x] = -1*cur_var*scaling_factor\n",
    "            if cur_skew > 0:\n",
    "                lookup_info[col1_id, col2_id, percent_id, 4, x] = cur_skew*scaling_factor\n",
    "            else:\n",
    "                lookup_info[col1_id, col2_id, percent_id, 5, x] = -1*cur_skew*scaling_factor\n",
    "    for i in range(12):\n",
    "        lookup_info[col1_id,col2_id,percent_id,i,:] = (lookup_info[col1_id,col2_id,percent_id,i,:] - lookup_info[col1_id,col2_id,percent_id,i,:].min()) / (lookup_info[col1_id,col2_id,percent_id,i,:].max() - lookup_info[col1_id,col2_id,percent_id,i,:].min()+eps)\n",
    "    lookup_info[col1_id,col2_id,percent_id,10,:] = 1-lookup_info[col1_id,col2_id,percent_id,8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/fsl-mtc-sample.csv')\n",
    "df = df.sample(n=2000)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "num_df = df.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_info = np.zeros((len(num_df.columns), len(num_df.columns), 10, 12, num_bins), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = num_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_work(task):\n",
    "    i,col1,j, col2 = task\n",
    "    bi_hist, xed, _ = np.histogram2d(num_df[col1], num_df[cols[j]], bins=num_bins)\n",
    "    xed = xed[:-1]\n",
    "    for percent in range(10,110,20):\n",
    "        getSliderData(col1, col2, percent, bi_hist, xed, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 'total_purchase_cost_($)', 1, 'avg_latency_(ms)']\n",
      "[0, 'total_purchase_cost_($)', 2, 'total_requests']\n"
     ]
    }
   ],
   "source": [
    "tasks = []\n",
    "for i,col1 in enumerate(cols):\n",
    "    for j,col2 in enumerate(cols):\n",
    "        if i!=j:\n",
    "            tasks.append([i,col1,j,col2])\n",
    "executor = ProcessPoolExecutor(max_workers=os.cpu_count())\n",
    "futures = []\n",
    "for task in tasks[:2]:\n",
    "    parallel_work(task)\n",
    "    print(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_info = np.load('lookup_info_penguins.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_info = np.nan_to_num(lookup_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(np.isnan(lookup_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8520"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(lookup_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64),\n",
       " array([], dtype=int64))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.logical_or(lookup_info>1, lookup_info<0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('lookup_info_cars.npy', lookup_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
